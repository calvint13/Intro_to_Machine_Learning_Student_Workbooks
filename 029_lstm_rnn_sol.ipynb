{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-tuner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pd.options.mode.chained_assignment = None\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "import keras\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  !pip install keras-tuner\n",
    "  import keras_tuner as kt\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  import keras_tuner as kt\n",
    "\n",
    "if not os.path.exists(\"logs\"):\n",
    "  !mkdir \"logs\"\n",
    "if not os.path.exists(\"logs/lstm_kt\"):\n",
    "  !mkdir \"logs/lstm_kt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  #plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "ticker = \"MSFT\"\n",
    "n_lookback = 60  # length of input sequences (lookback period)\n",
    "n_forecast = 10  # length of output sequences (forecast period)\n",
    "per = \"1y\"\n",
    "layer_size = 4\n",
    "batch = 4\n",
    "metrics = [\"mse\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "The RNN type of network and its uses mirror that of a CNN - while the convolutional neural networks excel at capturing spatial relationships in data, RNNs excel at capturing temporal relationships, or things that change over time. This leads to this type of network to be well suited to time series types of problems, and also things like text processing, where the words that occur earlier in a sentence are connected to those that occur later. \n",
    "\n",
    "![RNN](images/rnn.png \"RNN\" )\n",
    "\n",
    "The unique part of RNNs is that they can loop back, thus allowing the model to discover temporal relationships, or relationships that span over sequential data.\n",
    "\n",
    "### RNN Structure\n",
    "\n",
    "We will not focus much on a vanilla RNN model, we'll look at a variant - the Long Short-Term Memory model below. We should briefly look at the basics that make up a RNN. The structure of an RNN is similar to that of other neural networks, but with a few key differences:\n",
    "<ul>\n",
    "<li> The first is that the input is a sequence of data, rather than a single image. \n",
    "<li> The second is that the output is a sequence of data, rather than a single value. \n",
    "<li> The third is that the network has looping, which allows it to remember things that happened in the past.\n",
    "</ul>\n",
    "\n",
    "Clearly, the attributes of RNNs make them well suited to time series problems, where the input is a sequence of data, and the output is also a sequence of data; they are also heavily used in text processing and NLP problems, where the input is a sequence of words, and the output is also a sequence of words. Until relatively recently, RNNs were the state of the art for NLP problems, particularly generative models, but the advent of the Transformer architecture has made them less popular. \n",
    "\n",
    "### RNN Memory\n",
    "\n",
    "The defining characteristic of a RNN model is that it holds a \"state\", or a memory of the data that came from previous time steps. In NLP applications this state remembers information from what came previously in a sentence. In a time series application, this state remembers information from previous time steps. Just as the convolution operation in a CNN captures spatial relationships, the RNN operation captures temporal relationships with this state. As the model progresses through the sequential data, the state that it holds both informs the output and is updated by the data in the current time step.\n",
    "\n",
    "### Vanishing (or Exploding) Gradients\n",
    "\n",
    "A main reason that plain implementations of RNN models aren't used all that much is the problem of vanishing, or sometimes exploding, gradients. Recall that the gradient when we are doing gradient descent is the slope of the loss curve with respect to the weights, and we use that gradient along with the learning rate to adjust the weights up or down to make a great model. One problem with a RNN is that we need to also perform that gradient descent update of weights back through the time steps that make up our model. This means that the gradient is multiplied by the weights at each time step, and if the weights are small we can end up with gradients that are basically flat (no slope, or gradient to the curve) which can mean that the weights aren't really \"learning\", as they aren't being adjusted substantially as the model trains. This is called the vanishing gradient problem, the opposite, exploding gradients, is when the gradient is large and compounds over time. the problem of vanishing gradients is most easy to think of by thinking about NLP problems. If we have a sentence such as:\n",
    "<ul>\n",
    "<li> \"France is where I grew up, I speak fluent French.\"\n",
    "</ul>\n",
    "The early information that someone grew up in France will likely inform the language that they speak fluently. If our model is experiencing a vanishing gradient problem, it may not be able to reach \"fluent\" and then remember that France is in the memory, as the gradients going back through all the layers and all the time steps is so small that the problem of \"I speak SOME LANGUAGE fluently\" isn't able to be learned with the knowledge that \"they are from France\", as the small gradients aren't changing, or learning, all the way back to the early weights. We can visualize it like this, the early steps and the late steps have a connection that fades over time. \n",
    "\n",
    "![Vanishing Gradient](images/vanish_grad.jpeg \"Vanishing Gradient\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory\n",
    "\n",
    "Long Short Term Memory (lstm) models are a type of RNN that we can commonly use to make time series predictions. LSTM models function to \"remember\" certain data and carry that forward, and forget other data. This gives LSTM models the ability to combat the vanishing gradient problem and maintain a \"better memory\" than a plain implementation of a RNN. LSTM models work by holding a couple of \"states\", or memory, and control the flow of data through the network and into those states with a set of gates. These LSTM models are near state of the art for sequental problems, surpassed only recently by the Transformer architecture that is in large language models like the GPTs. \n",
    "\n",
    "![LSTM](images/lstm.png \"LSTM\" )\n",
    "\n",
    "LSTM models have some internal magic that allows them to remember data:\n",
    "<ul>\n",
    "<li> Cell state (C) - the \"long term memory\" of the model.\n",
    "<li> Hidden state (h) - the \"short term memory\" of the model.\n",
    "<li> Forget gate (1) - determines which old data can be dropped.\n",
    "<li> Input gate (2) - processes new data. \n",
    "<li> Output gate (3) - combines the \"held\" old data with the new data to generate the output. \n",
    "</ul>\n",
    "\n",
    "### LSTM States\n",
    "\n",
    "The LSTM model holds two types of memory, the long and the short term. Each step of the model's training involves updating both of these states with the new incoming data at that time step. Each state is passed from time step to time step through the model, and the new data is used to update the states by each gate. \n",
    "\n",
    "### LSTM Gates\n",
    "\n",
    "The flow of data into the states is controlled by a series of gates, each using either the tanh or the signmoid activation functions, strategically. Each activation function serves a different purpose here:\n",
    "<ul>\n",
    "<li> Sigmoid - acts to scale the importance of weights. \n",
    "<li> Tanh - acts to embed the current state in a normalized space. \n",
    "</ul>\n",
    "\n",
    "The gates are:\n",
    "<ul>\n",
    "<li> Forget gate - determines which data to forget. The forget gate uses the sigmoid activation function on the current input and the previous hidden state to determine which data to forget. This is passed up to the cell state and multiplied by the current cell state. This acts to raise values that are \"important\" and lower values that are \"unimportant\".\n",
    "<li> Input gate - determines which data to update. This works in conjunction with the \"candidate gate\", which is the tanh part - techincally it is its own thing, but it is effectively half of the input gate. The input gate generates both a representation and an importance scale of the hidden state and input data with the tanh and sigmoid activation functions. The representation is then multiplied by the importance scale and added to the current cell state. This \"adds\" the new data to the current cell state, but only the data that is \"important\" is added.\n",
    "<li> Output gate - determines which data to output. This takes a representation of the <i>new</i> cell state multiplied by the sigmoid activation function of the hidden state and input data. This takes the importance of the current time step combined with the understanding of the current state. This value is also what is ultimately output from the model, to go into the dense layers for the final prediction.\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Data\n",
    "\n",
    "We'll use the closing price as our target here, we can download the data from the yahoo finance API. One of the good things about using LSTM models is that they are much more accepting than the more basic ARIMA-ish models in terms of data preparation. We don't need to deconstruct the data to try to make it stationary as the model is much better able to capture the underlying patterns in the data. The ARIMA model is basically a linear regression model, which we know is not super great at capturing non-linear relationships. The LSTM model has no such restriction, and if we have enough data and a reasonably repeatable pattern in the data, we can likely capture that pattern in the model itself. Personally, I find all the elaborate time series data preparation to be a pain, using the LSTM appoach makes it much more similar to regular regression/classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "df = yf.download(tickers=[ticker], period=per)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Close'].fillna(method='ffill')\n",
    "y = y.values.reshape(-1, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(y)\n",
    "y = scaler.transform(y)\n",
    "print(y[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Data for Predictions\n",
    "\n",
    "For each point in our data we generate additional data at that point. LSTM models expect data to always be a 3D tensor of the dimensions:\n",
    "<ul>\n",
    "<li> [batch_size, timesteps, feature]\n",
    "</ul>\n",
    "\n",
    "The outcome of this will be data where each time point is now no longer just 1 value, like a normal time series, but it is a bunch of dates - this  is the \"long term\" memory part. The X's shape is:\n",
    "<ul>\n",
    "<li> (# of rows of data, # of lookback rows, number of features)\n",
    "<li> I.e. each \"row\" now has the past 60 values as part of it. \n",
    "</ul>\n",
    "\n",
    "The Y's shape is:\n",
    "<ul>\n",
    "<li> (# of rows of data, # of forecasting rows, number of features)\n",
    "<li> I.e. each row is a prediction into the future. \n",
    "</ul>\n",
    "\n",
    "Each of the inputs for our data \"remembers\" the past 60 days into the past! This is what allows these models to do such a good job on sequential data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the input and output sequences\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(n_lookback, len(y)):\n",
    "    X.append(y[i - n_lookback: i])\n",
    "    y_tmp = y[i: i + n_forecast]\n",
    "    Y.append(y_tmp[0][0])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Model\n",
    "\n",
    "Our data is now ready to be used to fit a model, importantly, each X value is now a sequence containing the data of the past 60 days, rather than \"just\" a single value. So our input shape is now (60, 1). Each prediction takes in one value, and 60 time steps of it in the past; each prediction is a single value, and one time step of it. \n",
    "\n",
    "We can now make a model and train it. The long-short term memory layers are mostly simple to use, we need to be aware of a few things:\n",
    "<ul>\n",
    "<li> Input shape - this input shape is the size of the previous records and the number of features. Here we have 60 for the n_lookback and 1 for the number of features. \n",
    "<li> Return Sequences - this controls if the LSTM layers return everything or just the end result. We want all layers that are feeding another LSTM layer to return sequences, but the last LSTM layer should not.\n",
    "<li> Output - each forecast gets its own output neuron, we are predicting exactly one time step value. \n",
    "</ul>\n",
    "\n",
    "The units is our size parameter for each layer. Like in other models, the larger the size, the more complex the model can be, but the more likely it is to overfit. There are a few more notes on sizing below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=layer_size, return_sequences=True, input_shape=(n_lookback, 1), dropout=0.2))\n",
    "model.add(LSTM(units=layer_size, dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n",
    "history = model.fit(X, Y, epochs=epochs, batch_size=batch, verbose=1)\n",
    "tmp = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print\n",
    "old_preds = []\n",
    "for i in range(len(tmp)):\n",
    "    old_preds.append(tmp[i][0])\n",
    "\n",
    "# organize the results in a data frame\n",
    "df_past = df[['Close']].reset_index()\n",
    "#print(len(df_past))\n",
    "df_past.rename(columns={'index': 'Date', 'Close': 'Actual'}, inplace=True)\n",
    "df_past['Date'] = pd.to_datetime(df_past['Date'])\n",
    "\n",
    "df_past['Old'] = np.nan\n",
    "for i in range(len(old_preds)):\n",
    "    df_past[\"Old\"].iloc[i+n_lookback-1] = scaler.inverse_transform(np.array(old_preds[i]).reshape(1,-1))\n",
    "\n",
    "results = df_past.set_index('Date')\n",
    "# plot the results\n",
    "plot_loss(history)\n",
    "results.plot(title=ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_past[~df_past[\"Old\"].isna()]\n",
    "trainScore = math.sqrt(mean_squared_error(df_tmp[\"Actual\"], df_tmp[\"Old\"]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Forecasts - Model with Larger Y\n",
    "\n",
    "We can also try to predict more than one day into the future as a core feature of our model. With stats-based models, if we want to project multiple time steps into the future we have to use our initial predictions to feed the later predictions. With a LSTM we can generate a standard output prediction that is multiple timesteps to begin with. To do this we need to change the shape of the Y data that is being predicted, rather than predicting one value, we can create a Y set that is the next 10 values, giving us 2 weeks of price forecasts. So, for each date in the data, we have:\n",
    "<ul>\n",
    "<li> X data this is the past 60 days of data. \n",
    "<li> Y data this is the next 10 days of data.\n",
    "</ul>\n",
    "\n",
    "These two values are the middle dimension of our data, the number of time steps. So, we need to make our dataset to model this - each prediction is now a sequence of 10 values, rather than a single value. Our 10 day future prediction isn't a run of the \"next 10\" predictions that come from the end of our model, they are one unitary output of our model that is 10 values long! Note that we can use the other method of feeding in predictions that we generate as a source for the next time-step's prediction, but this appoach is more elegant, normally easier to implement, and makes our model more \"complete\" in the sense that it will always generate its full output in one go. This may or may not be more accurate, that is not assured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stockPredPrep(data, column=\"Close\", n_lookback=60, n_forecast=10):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    y = data[[column]].fillna(method='ffill')\n",
    "    y = scaler.fit_transform(y)\n",
    "    start = n_lookback\n",
    "    end = len(y) - n_forecast\n",
    "    #end = len(y)\n",
    "    X = []\n",
    "    Y = []\n",
    "    print(start,end)\n",
    "    for i in range(start, end):\n",
    "        X.append(y[i - n_lookback: i])\n",
    "        y_tmp = y[i: i + n_forecast]\n",
    "        Y.append(np.array(y_tmp))\n",
    "    return np.array(X), np.array(Y), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p, Y_p, scaler_p = stockPredPrep(df, n_lookback=n_lookback, n_forecast=n_forecast)\n",
    "print(X_p.shape, Y_p.shape)\n",
    "print(Y_p[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Predictions\n",
    "\n",
    "Each y value is now a sequence of 10 values - the next 10 days of price. The output layer of our network also needs to be rearranged to match this, with one neuron for each prediction that is part of one Y value. This also changes the way the model is being trained, as now the 10 timestep prediction is the thing that is being evaluated for loss. We are technically not creating the model that minimizes the loss of one prediction, we are creating the model that minimizes the loss of the 10 predictions. Whether this is good or bad depends somewhat on perspective. It does make the structure of the model easier to use, as we don't need to feed predictions back into a model to get longer range predictions. This note is important, in a normal time-series model we were always generating predictions for the next time step in some ever-rolling manner if we wanted to make projections farther into the future (potentially outside of the Facebook Prohet models, which are more sophisticated internally). This means that there was no direct relationship in those models between \"now\" and \"2 weeks from now\" in the model, we were basically trusting that our \"tomorrow\" predictions would stack up to be accurate into the future. This is also a reason that deconstructing the trends and cyclicality of the data in those time series models was very important, we needed that so the model knew how to structure the patterns of its predictions.\n",
    "\n",
    "In a multi-step prediction with our LSTM model we can construct our target for the model itself to by \"N timesteps\" rather than the next time step, like we are used to. This means that we are always generating a \"N step\" prediction and always evaluting the accuracy of those N steps when calculating the loss and optimizing our model. The result here is a \"real\" prediction that \"based on the current information at this time step, we expect the value N steps ahead to be M\", and that prediction will be based on the accuracy of the previous \"N days out\" predictions that we've made during training. Combined with the large capacity of the models and the ability to construct deep models, this opens the door to the accurate \"far out\" forecasting that models are capable of, given substantial data. This isn't a guarantee that these multi-step predictions will be the most accurate, we are still predicting something that hasn't happened yet, and reality is tricky. This is also one of the most relatable reasons I can think of to define a custom loss function, the way we treat error for predictions of tomorrow vs predictions of 2 weeks from now can reasonably be very different in different scenarios. For example, a stock price forecasting model needs to be very accuracte (potentially less than a cent) for predictions used to make automated high frequency trading decisions, but it doesn't need to be as accurate for predictions of stocks that are likely to climb over the next year. Defining a custom loss function that evaluates the accuracy of a model's predictions based on the real world impact of those errors is pretty likely if a time series model is fit into a very specific situation. \n",
    "\n",
    "#### Modelling Options\n",
    "\n",
    "We can also try adding some more complexity to the model, just like any other network. One note is that if we add more lstm layers, we need to turn on the return sequences flag, so we are passing all the data to the next layer. This is basically a \"keep LSTMing\" or \"stop LSTMing\" flag that we always leave off on only the final LSTM layer and never think about again. There are a few other things that we <i>could</i> do with a model using this and another similar option, return_state, that returns the hidden state information. These can be used to develop customized architectures when using LSTM in functional models - the kind that we can customize almost limitlessly. Doing this is generally both complex and context dependent, for example, if we were developing audio recognition models for a company like Shazam (phone app that hears a song and looks it up for you) we may want to take advantage of things like this to create a model that is specifically good at exactly what we want to do. The Shazam model needs to extract a song from other audio like talking in an often noisy environment, process the audio that has been \"distorted\" by different audio systems (e.g. nightclub, car radio, tiny built in TV speakers...), match it to one of all other songs (bazillion way softmax classification?) based on any small clip, and do all of this within a couple of seconds. I am definately not in charge of audio engineering at Shazam, but I can go out on a pretty stable limb and say that they have models that are highly tailored to specific aspects of audio processing that are far more important for the odd challenge they have at hand. Long story short, this is one of the things that one could use to create such types of models, but it goes beyond our scope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=layer_size, return_sequences=True, input_shape=(n_lookback, 1)))\n",
    "model.add(LSTM(units=layer_size))\n",
    "model.add(Dense(10))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n",
    "history = model.fit(X_p, Y_p, epochs=epochs, batch_size=batch, verbose=1)\n",
    "tmp = model.predict(X_p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check a Prediction\n",
    "\n",
    "We can check a prediction to see what it looks like. The final prediction is now 10 numbers long, we have 10 predictions for the next 10 days!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_p.inverse_transform(tmp[-1].reshape(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Predictions\n",
    "\n",
    "Because we have a bunch of predictions for each individual record, we need to collapse them together somehow to be able to make a simple plot of our predictions. There are several ways to do this, depending on what you want, but here we will just take the average of the predictions for each day. This gives us one value for each day, which we can then plot. You might alternatively want to take the most recent, median, or a weighted average of the predictions - the full stacks of predictions is also returned, so alternate calculations are possible. We haven't investigated it here, but we are likely seeing some fairly inaccurate predictions for the predictions made 9 or 10 days in advance being averaged with some pretty accurate predictions for the predictions made 1 or 2 days in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergePreds(preds, forecasts):\n",
    "    tmp_preds = []\n",
    "    mean_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        end = i\n",
    "        start = end - forecasts\n",
    "        item_preds = []\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        record = forecasts-1\n",
    "        for n in range(start, end):\n",
    "            item = preds[n][record]\n",
    "            #print(item, item.shape, type(item))\n",
    "            item_preds.append(item)\n",
    "            record -= 1\n",
    "        if len(item_preds) > 1:\n",
    "            tmp_preds.append(item_preds)\n",
    "            mean_preds.append(np.mean(item_preds))\n",
    "    return tmp_preds, mean_preds\n",
    "\n",
    "def plotStockPred(preds, real, scaler, y_size=10):\n",
    "    # organize the results in a data frame\n",
    "    df_past = real[['Close']].reset_index()\n",
    "    #print(len(df_past))\n",
    "    df_past.rename(columns={'index': 'Date', 'Close': 'Actual'}, inplace=True)\n",
    "    df_past['Date'] = pd.to_datetime(df_past['Date'])\n",
    "\n",
    "    df_past['Old'] = np.nan\n",
    "    for i in range(len(preds)):\n",
    "        #df_past[\"Old\"].iloc[i+n_lookback-1] = scaler.inverse_transform(np.array(preds[i]).reshape(1,-1))\n",
    "        val = scaler.inverse_transform(preds[i].reshape(1,-1))[0][0]\n",
    "        #print(i, val)\n",
    "        df_past[\"Old\"].iloc[i+n_lookback-1] = val\n",
    "\n",
    "    results = df_past.set_index('Date')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_preds, mean_preds = mergePreds(tmp, n_forecast)\n",
    "\n",
    "res = plotStockPred(mean_preds, df, scaler_p, n_forecast)\n",
    "res.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Predictions\n",
    "\n",
    "We can make predictions into the future, we just need to feed the model the data up to the most recent record, and then have it predict the next 10 days. Our input X value for each prediciton is one record of data, which we've seen contains the past 60 days of data. To make a prediction, we need to assemble our \"last day\" row of data that contains the previous 60 days worth of data that we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = stockPredPrep(df, n_forecast=0)\n",
    "future_data[0][-1] # The last record"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape and Predict\n",
    "\n",
    "Mucho reshaping - we are getting the most recent set of data to predict for, since that'll give us a prediciton 10 days into the future. We are also reshaping that data - one batch, \"however many\" time steps, and one feature. So our shapes are:\n",
    "<ul>\n",
    "<li> X shape: (1, 60, 1) - one batch, 60 time steps, 1 feature\n",
    "<li> Y shape: (1, 10, 1) - one batch, 10 time steps, 1 feature\n",
    "</ul>\n",
    "\n",
    "Both the X and Y are multidimensional here, but this is one record as far as the model is concerned. Similar to how with CNN models each record is a 2D (plus color depth) image, here each record is a 3D tensor. To make a prediction, we feed the X and get the Y, just as we are used to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_preds = model.predict(future_data[0][-1].reshape(1,-1,1))\n",
    "future_preds = scaler_p.inverse_transform(future_preds.reshape(-1, 1))\n",
    "future_preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine with the old data. I will stack the original data together with the new predictions, and then plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df[\"Close\"]\n",
    "df_pred = df_pred.to_frame()\n",
    "df_pred[\"Date\"] = df_pred.index\n",
    "\n",
    "df_tmp = pd.DataFrame(columns=[\"Date\", \"Close\"])\n",
    "df_tmp[\"Date\"] = pd.date_range(start=df_pred[\"Date\"].iloc[-1] + pd.Timedelta(days=1), periods=len(future_preds))\n",
    "df_tmp[\"Close\"] = future_preds\n",
    "\n",
    "df_pred.set_index(\"Date\", inplace=True)\n",
    "df_tmp.set_index(\"Date\", inplace=True)\n",
    "\n",
    "#results = df_pred.append(df_tmp)\n",
    "results = pd.concat([df_pred, df_tmp])\n",
    "results.tail(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot It\n",
    "\n",
    "I am going to just shade the prediction part on the plot here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_tmp.index[0]\n",
    "e = df_tmp.index[-1]\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "results.plot(ax=fig.gca())\n",
    "plt.axvspan(s,e, color='g', alpha=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets with More Features\n",
    "\n",
    "We can add more features to the model as well. One of the benefits to using a neural network is that adding more features does not require the same work and effort as with traditional time series models. Since we are generating a prediciton of N days into the future at any given time, we aren't restricted to the values that \"we can know for the future\" or offset calculations like we had to do with the ordinary time series models. The increased capacity of neural networks to learn relationships means that our model will accept more data with few changes.\n",
    "\n",
    "Reconstructing the data to accomodate more features is a bit more cumbersome, we need to create data that is:\n",
    "<ul>\n",
    "<li> Y - the next N days of data. \n",
    "    <ul>\n",
    "    <li> Shape - batch size, number of timesteps to predict, 1 value (usually). \n",
    "    </ul>\n",
    "<li> X - the past N days of data, plus the features that we want to use.\n",
    "    <ul>\n",
    "    <li> Shape - batch size, number of timesteps to use (M days), number of features.\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "So for any one prediction, the X input is the M day lookback of both the price and all the other features. The Y output is the next N days of price.\n",
    "\n",
    "#### Nicer Data Prep Function\n",
    "\n",
    "We can pair this with a more generalized function to prep our data. This one should, assuming I didn't mess it up, allow us to use any number of features and set the time windows to look both forward and backward to whatever we want. The columns parameter limits the data to those columns, and the y_col specifies which of these is the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_stock_data(ticker, columns, y_col=\"Close\", lookback=60, lookahead=10, timeframe=\"5y\"):\n",
    "    # Download stock prices for the past 5 years\n",
    "    data = yf.download(ticker, period=timeframe)\n",
    "\n",
    "    # Select the desired columns\n",
    "    data = data[columns]\n",
    "\n",
    "    # Prepare the data for an LSTM model\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(lookback, len(data)-lookahead):\n",
    "        x.append(data.iloc[i-lookback:i].values)\n",
    "        y.append(data[y_col][i:i+lookahead].values)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Reshape the data for the LSTM model\n",
    "    x = np.reshape(x, (x.shape[0], x.shape[1], x.shape[2]))\n",
    "    y = np.reshape(y, (y.shape[0], y.shape[1], 1))\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"real\" predictions using the open, low, and close values doesn't make a lot of sense, but for a demonstration it's fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "\n",
    "x_tmp, y_tmp = prepare_stock_data(\"AAPL\", columns=columns, y_col=\"Close\", lookback=n_lookback, lookahead=n_forecast)\n",
    "print(x_tmp.shape, y_tmp.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit Model\n",
    "\n",
    "The input shape for X is different now:\n",
    "<ul>\n",
    "<li> Batch size, by...\n",
    "<li> Number of lookback timesteps (M days), by...\n",
    "<li> Number of features (price + volume + whatever else)\n",
    "</ul>\n",
    "\n",
    "The output shape remains the same as before, we are still predicting 10 days into the future, so we output to 10 neurons and each output is a list of 10 individual predictions of one single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=layer_size, return_sequences=True, input_shape=(n_lookback, len(columns)), dropout=0.2))\n",
    "model.add(LSTM(units=layer_size, dropout=0.2))\n",
    "model.add(Dense(n_forecast))\n",
    "\n",
    "weight_url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/lstm_model(1).keras\"\n",
    "try:\n",
    "    old_weights = keras.utils.get_file(\"weights\", weight_url)\n",
    "    model.load_weights(old_weights)\n",
    "    print(\"Weights loaded\")\n",
    "except:\n",
    "    print(\"No weights found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n",
    "history = model.fit(x_tmp, y_tmp, epochs=epochs, batch_size=batch, verbose=0)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"lstm_model.keras\"\n",
    "model.save_weights(save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - Tuning Performance\n",
    "\n",
    "Like any other neural network model, we can change some things to try to improve the performance of our model. Like any other network, the true test of our model will come down to experimentation and testing. \n",
    "\n",
    "#### Unit Size and Layers\n",
    "\n",
    "The \"number of neurons\" argument in creating an LSTM layer is the number of units in the layer. This parameter defines the size of the hidden state vector that is passed between the layers and combines with the size of the input to define the rest of the sizes of the weight matricies. The units is not related to the time steps, it is better though of as roughly the size of the data that we are processing at each time step. This works like the number of neurons in a normal dense layer, the larger the value, the higher the capacity of that layer to learn. \n",
    "\n",
    "Adding layers to the LSTM model is the other general strategy to creating a more powerful model - the more layers we add, the more complex the model can be, and the more it can learn. The basic structure of an LSTM model is similar to what we saw in a CNN, we have the \"specialized stuff\" in the first layers, either LSTM or CNN, then we add on some amount to dense layers to generate a final prediction. Models that have multiple LSTM layers are often called \"stacked LSTM\" models.\n",
    "\n",
    "As with dense models, we need to decide how large to make our model in capacity, and how to balance that betweeen the two ways to add capacity - \"width\", or the number of units in each layer, and \"depth\", or the number of layers. This is something that ultimately needs to be tested to be firmly determined, but we can use some rules of thumb to give us good starting points:\n",
    "<ul>\n",
    "<li> If we have a \"simple\" pattern, like the examples that were easy to predict with moving average based strategies, one LSTM layer should be enough, likely with fewer than 50 units, maybe far fewer. \n",
    "<li> For more complex patterns:\n",
    "    <ul>\n",
    "    <li> A good starting point is to use about 50 units the first LSTM layer. If the problem appears complex, and maybe one other layer with a similar number of units.\n",
    "    <li> If the patterns in the data appear to be largely defined by <b>interactions between multiple input features</b> or by <b>pattern blocks</b>, such as those identified by technical stock trading above, then we should try more units. \n",
    "    <li> If the patterns in the data appear to be <b>highly abstract</b>, such as parts of speech/language being components of full sentences, then we should try more layers. This lines up with very deep models being good at things like language translation, the structure and capacity of the model is well suited to the problem of language, with very abstract relationships. This is conceptually similar to how deeper CNN models are able to transform images through multiple layers and extract useful features on which to base a prediction - the deep LSTM models are able to transform temporal data through multiple layers and extract useful features.\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "The relative balance between the two ways to add capacity can't be too far out of balance, which is a guideline without specifics; we are less likely to see good results with an LSTM layer of 3,000 units and one layer, or 2 units and 75 layers. In general, things that are based on unstructured data such as NLP based applications will tend to be better suited with a deep model, things like a sales forecast can likely be well served by a more shallow model. The training times for very deep models with large timesteps can get very large, as can the memory requirements. There is a rule of thumb that we can use for an estimate for a total number of units, but it is, at best, a very rough guideline:\n",
    "<ul>\n",
    "<li> Units = samples in training data / ((# input neurons + # output neurons) * alpha)\n",
    "<li> Where alpha is a constant normally between 2 and 10, with 5 being a good starting point.\n",
    "</ul>\n",
    "\n",
    "So an algorithmic approach to deciding on a rough starting size for a model would be to calculate the total neurons above, using an alpha that mirrors the complexity, then divide that by one layer for a simple problem, two or more for a complex problem. The farther we get from a simple time-series problem, the less likely this rule of thumb will be precise. \n",
    "\n",
    "#### Dropouts in LSTM\n",
    "\n",
    "Dropout layers are commonly used in LSTM models as a way to prevent overfitting. There are two basic ways that dropout layers can be used in an LSTM model:\n",
    "<ul>\n",
    "<li> Dropout between LSTM layers. \n",
    "<li> Dropout after LSTM layers, before the output layer. \n",
    "</ul>\n",
    "\n",
    "Of the two, placing the dropout between LSTM layers will generally give more variance in the impact of the layer - potentially in both directions. There are also many other varieties of dropout strategies being researched, all seeking to be situationally smarter variations of the standard dropout layer we've applied here. While dropout layers are extremely common to see in LSTM models, they are not certain to improve model performance and their usefulness needs to be tested to be truly evaluated.\n",
    "\n",
    "#### Keras Tuner\n",
    "\n",
    "We can use the Keras Tuner to do some hyperparameter tuning for the best configuration. The same principles apply as with any other model, we need to define a search space, and then run the search. One thing that we can do to make our search space smaller and allow more time for productive search, we can likely create smaller models that visually seem to \"follow the trend\" if we plot the training data - take that model that looks decent, and search +/- some units, +/- some layers, and try some variations such as dropouts or an additional dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_unit',min_value=4,max_value=128,step=4),return_sequences=True, input_shape=(n_lookback, len(columns))))\n",
    "    for i in range(hp.Int('n_layers', 1, 3)):\n",
    "        model.add(LSTM(hp.Int(f'lstm_{i}_units',min_value=4,max_value=128,step=4),return_sequences=True))\n",
    "    model.add(LSTM(25))\n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(Dense(10))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timstamp = str(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "proj = \"lstm_kt\" + timstamp\n",
    "bayesian_opt_tuner = kt.tuners.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='mse',\n",
    "    max_trials=3,\n",
    "    executions_per_trial=3,\n",
    "    directory=\"logs/lstm_kt\",\n",
    "    project_name=proj,\n",
    "    overwrite=True)\n",
    "bayesian_opt_tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this for a more realistic search, but much longer. \n",
    "#search_epochs = int(epochs/3)\n",
    "search_epochs = 3\n",
    "bayesian_opt_tuner.search(x_tmp, y_tmp,epochs=search_epochs,validation_split=0.2,verbose=1, batch_size=batch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Complete - Results\n",
    "\n",
    "We can take a look at the results and take the best model to go do our predicting... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = bayesian_opt_tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Predict the temperature for the next 24 hours, the target column is \"T (degC)\". Use the past 30 days of data to do so as a starting point, if you run into resource issues, switch that to 15 or 7 and give it a try. If you have time, attempt to make a function that constructs both the X and Y data for you, and can take in a number of days to look back, a number of days to predict forward, and ideally, a list of columns to use as features. As well, test one feature vs a bunch of features, and see if you can get a better result with more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Time</th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>wd (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2009 00:10:00</td>\n",
       "      <td>996.52</td>\n",
       "      <td>-8.02</td>\n",
       "      <td>265.40</td>\n",
       "      <td>-8.90</td>\n",
       "      <td>93.3</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1307.75</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.75</td>\n",
       "      <td>152.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2009 00:20:00</td>\n",
       "      <td>996.57</td>\n",
       "      <td>-8.41</td>\n",
       "      <td>265.01</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>93.4</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.89</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1309.80</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.50</td>\n",
       "      <td>136.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2009 00:30:00</td>\n",
       "      <td>996.53</td>\n",
       "      <td>-8.51</td>\n",
       "      <td>264.91</td>\n",
       "      <td>-9.31</td>\n",
       "      <td>93.9</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1310.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.63</td>\n",
       "      <td>171.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.01.2009 00:40:00</td>\n",
       "      <td>996.51</td>\n",
       "      <td>-8.31</td>\n",
       "      <td>265.12</td>\n",
       "      <td>-9.07</td>\n",
       "      <td>94.2</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1309.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2009 00:50:00</td>\n",
       "      <td>996.51</td>\n",
       "      <td>-8.27</td>\n",
       "      <td>265.15</td>\n",
       "      <td>-9.04</td>\n",
       "      <td>94.1</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1309.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.63</td>\n",
       "      <td>214.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01.01.2009 01:00:00</td>\n",
       "      <td>996.50</td>\n",
       "      <td>-8.05</td>\n",
       "      <td>265.38</td>\n",
       "      <td>-8.78</td>\n",
       "      <td>94.4</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.96</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1307.86</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.63</td>\n",
       "      <td>192.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01.01.2009 01:10:00</td>\n",
       "      <td>996.50</td>\n",
       "      <td>-7.62</td>\n",
       "      <td>265.81</td>\n",
       "      <td>-8.30</td>\n",
       "      <td>94.8</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.04</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1305.68</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.63</td>\n",
       "      <td>166.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>01.01.2009 01:20:00</td>\n",
       "      <td>996.50</td>\n",
       "      <td>-7.62</td>\n",
       "      <td>265.81</td>\n",
       "      <td>-8.36</td>\n",
       "      <td>94.4</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1305.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.50</td>\n",
       "      <td>118.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>01.01.2009 01:30:00</td>\n",
       "      <td>996.50</td>\n",
       "      <td>-7.91</td>\n",
       "      <td>265.52</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>93.8</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.97</td>\n",
       "      <td>3.16</td>\n",
       "      <td>1307.17</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.75</td>\n",
       "      <td>188.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01.01.2009 01:40:00</td>\n",
       "      <td>996.53</td>\n",
       "      <td>-8.43</td>\n",
       "      <td>264.99</td>\n",
       "      <td>-9.34</td>\n",
       "      <td>93.1</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1309.85</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.88</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
       "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
       "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
       "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
       "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
       "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
       "5  01.01.2009 01:00:00    996.50     -8.05    265.38        -8.78    94.4   \n",
       "6  01.01.2009 01:10:00    996.50     -7.62    265.81        -8.30    94.8   \n",
       "7  01.01.2009 01:20:00    996.50     -7.62    265.81        -8.36    94.4   \n",
       "8  01.01.2009 01:30:00    996.50     -7.91    265.52        -8.73    93.8   \n",
       "9  01.01.2009 01:40:00    996.53     -8.43    264.99        -9.34    93.1   \n",
       "\n",
       "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
       "0          3.33          3.11          0.22       1.94             3.12   \n",
       "1          3.23          3.02          0.21       1.89             3.03   \n",
       "2          3.21          3.01          0.20       1.88             3.02   \n",
       "3          3.26          3.07          0.19       1.92             3.08   \n",
       "4          3.27          3.08          0.19       1.92             3.09   \n",
       "5          3.33          3.14          0.19       1.96             3.15   \n",
       "6          3.44          3.26          0.18       2.04             3.27   \n",
       "7          3.44          3.25          0.19       2.03             3.26   \n",
       "8          3.36          3.15          0.21       1.97             3.16   \n",
       "9          3.23          3.00          0.22       1.88             3.02   \n",
       "\n",
       "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
       "0       1307.75      1.03           1.75     152.3  \n",
       "1       1309.80      0.72           1.50     136.1  \n",
       "2       1310.24      0.19           0.63     171.6  \n",
       "3       1309.19      0.34           0.50     198.0  \n",
       "4       1309.00      0.32           0.63     214.3  \n",
       "5       1307.86      0.21           0.63     192.7  \n",
       "6       1305.68      0.18           0.63     166.5  \n",
       "7       1305.69      0.19           0.50     118.6  \n",
       "8       1307.17      0.28           0.75     188.5  \n",
       "9       1309.85      0.59           0.88     185.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\"\n",
    "zip_path = keras.utils.get_file(origin=uri, fname=\"jena_climate_2009_2016.csv.zip\")\n",
    "zip_file = ZipFile(zip_path)\n",
    "zip_file.extractall()\n",
    "csv_path = \"jena_climate_2009_2016.csv\"\n",
    "\n",
    "weather_dataframe = pd.read_csv(csv_path)\n",
    "#weather_dataframe[\"Date Time\"] = pd.to_datetime(weather_dataframe[\"Date Time\"])\n",
    "weather_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Time</th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>wd (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>420541</th>\n",
       "      <td>31.12.2016 22:30:00</td>\n",
       "      <td>1000.44</td>\n",
       "      <td>-4.08</td>\n",
       "      <td>269.05</td>\n",
       "      <td>-7.89</td>\n",
       "      <td>74.60</td>\n",
       "      <td>4.51</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.10</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1293.55</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.48</td>\n",
       "      <td>192.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420542</th>\n",
       "      <td>31.12.2016 22:40:00</td>\n",
       "      <td>1000.45</td>\n",
       "      <td>-4.45</td>\n",
       "      <td>268.68</td>\n",
       "      <td>-7.15</td>\n",
       "      <td>81.30</td>\n",
       "      <td>4.39</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1295.24</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.44</td>\n",
       "      <td>183.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420543</th>\n",
       "      <td>31.12.2016 22:50:00</td>\n",
       "      <td>1000.32</td>\n",
       "      <td>-4.09</td>\n",
       "      <td>269.05</td>\n",
       "      <td>-7.23</td>\n",
       "      <td>78.60</td>\n",
       "      <td>4.51</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.21</td>\n",
       "      <td>3.54</td>\n",
       "      <td>1293.37</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.60</td>\n",
       "      <td>199.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420544</th>\n",
       "      <td>31.12.2016 23:00:00</td>\n",
       "      <td>1000.21</td>\n",
       "      <td>-3.76</td>\n",
       "      <td>269.39</td>\n",
       "      <td>-7.95</td>\n",
       "      <td>72.50</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.09</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1291.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.30</td>\n",
       "      <td>223.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420545</th>\n",
       "      <td>31.12.2016 23:10:00</td>\n",
       "      <td>1000.11</td>\n",
       "      <td>-3.93</td>\n",
       "      <td>269.23</td>\n",
       "      <td>-8.09</td>\n",
       "      <td>72.60</td>\n",
       "      <td>4.56</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.31</td>\n",
       "      <td>1292.41</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>202.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420546</th>\n",
       "      <td>31.12.2016 23:20:00</td>\n",
       "      <td>1000.07</td>\n",
       "      <td>-4.05</td>\n",
       "      <td>269.10</td>\n",
       "      <td>-8.13</td>\n",
       "      <td>73.10</td>\n",
       "      <td>4.52</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1292.98</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.52</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420547</th>\n",
       "      <td>31.12.2016 23:30:00</td>\n",
       "      <td>999.93</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>269.81</td>\n",
       "      <td>-8.06</td>\n",
       "      <td>69.71</td>\n",
       "      <td>4.77</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.07</td>\n",
       "      <td>3.32</td>\n",
       "      <td>1289.44</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.92</td>\n",
       "      <td>234.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420548</th>\n",
       "      <td>31.12.2016 23:40:00</td>\n",
       "      <td>999.82</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>270.01</td>\n",
       "      <td>-8.21</td>\n",
       "      <td>67.91</td>\n",
       "      <td>4.84</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1288.39</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.00</td>\n",
       "      <td>215.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420549</th>\n",
       "      <td>31.12.2016 23:50:00</td>\n",
       "      <td>999.81</td>\n",
       "      <td>-4.23</td>\n",
       "      <td>268.94</td>\n",
       "      <td>-8.53</td>\n",
       "      <td>71.80</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.99</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1293.56</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.16</td>\n",
       "      <td>225.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420550</th>\n",
       "      <td>01.01.2017 00:00:00</td>\n",
       "      <td>999.82</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>268.36</td>\n",
       "      <td>-8.42</td>\n",
       "      <td>75.70</td>\n",
       "      <td>4.27</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.01</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1296.38</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.96</td>\n",
       "      <td>184.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  \\\n",
       "420541  31.12.2016 22:30:00   1000.44     -4.08    269.05        -7.89   \n",
       "420542  31.12.2016 22:40:00   1000.45     -4.45    268.68        -7.15   \n",
       "420543  31.12.2016 22:50:00   1000.32     -4.09    269.05        -7.23   \n",
       "420544  31.12.2016 23:00:00   1000.21     -3.76    269.39        -7.95   \n",
       "420545  31.12.2016 23:10:00   1000.11     -3.93    269.23        -8.09   \n",
       "420546  31.12.2016 23:20:00   1000.07     -4.05    269.10        -8.13   \n",
       "420547  31.12.2016 23:30:00    999.93     -3.35    269.81        -8.06   \n",
       "420548  31.12.2016 23:40:00    999.82     -3.16    270.01        -8.21   \n",
       "420549  31.12.2016 23:50:00    999.81     -4.23    268.94        -8.53   \n",
       "420550  01.01.2017 00:00:00    999.82     -4.82    268.36        -8.42   \n",
       "\n",
       "        rh (%)  VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  \\\n",
       "420541   74.60          4.51          3.37          1.15       2.10   \n",
       "420542   81.30          4.39          3.57          0.82       2.22   \n",
       "420543   78.60          4.51          3.54          0.96       2.21   \n",
       "420544   72.50          4.62          3.35          1.27       2.09   \n",
       "420545   72.60          4.56          3.31          1.25       2.06   \n",
       "420546   73.10          4.52          3.30          1.22       2.06   \n",
       "420547   69.71          4.77          3.32          1.44       2.07   \n",
       "420548   67.91          4.84          3.28          1.55       2.05   \n",
       "420549   71.80          4.46          3.20          1.26       1.99   \n",
       "420550   75.70          4.27          3.23          1.04       2.01   \n",
       "\n",
       "        H2OC (mmol/mol)  rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
       "420541             3.37       1293.55      1.27           2.48     192.1  \n",
       "420542             3.57       1295.24      0.80           1.44     183.8  \n",
       "420543             3.54       1293.37      1.25           1.60     199.2  \n",
       "420544             3.35       1291.71      0.89           1.30     223.7  \n",
       "420545             3.31       1292.41      0.56           1.00     202.6  \n",
       "420546             3.30       1292.98      0.67           1.52     240.0  \n",
       "420547             3.32       1289.44      1.14           1.92     234.3  \n",
       "420548             3.28       1288.39      1.08           2.00     215.2  \n",
       "420549             3.20       1293.56      1.49           2.16     225.8  \n",
       "420550             3.23       1296.38      1.23           1.96     184.9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_dataframe.tail(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Data\n",
    "\n",
    "The data has records every 10 minutes, meaning each day is 6 * 24 = 144 records. We have many features that we could use to generate a model. I think that making a prediction for every 10 minutes is a bit much, so I am going to collapse the 6 rows per hour down to 1. This is an arbitrary choice that does surrender some information, but it will make our data much more manageable. With a massive amount of data that might cause us issues later, this also may help with resource usage...\n",
    "\n",
    "I'm going to construct my datasets to be in this structure:\n",
    "<ul>\n",
    "<li> X - batch size, 720 time steps (30 days), 13 features. \n",
    "    <ul>\n",
    "    <li> Each time step is the average of 6 individual time steps. \n",
    "    </ul>\n",
    "<li> Y - batch size, 24 time steps (to predict - 1 day), 1 feature (temperature).\n",
    "</ul>\n",
    "\n",
    "You could structure the dataset differently here, but make sure that what is going in and out makes sense here, and that you can roughly picture what would change if we were to make some changes to our choices:\n",
    "<ul>\n",
    "<li> What if I wanted to predict the next 48 hours instead of 24?\n",
    "<li> What if I wanted to predict the temperature every 30 minutes instead of every 10 minutes?\n",
    "<li> What if I wanted to base my predictions on the past 60 days instead of 30?\n",
    "<li> What if I wanted to use only wind velocity and barometric pressure as my features, instead of everything?\n",
    "</ul>\n",
    "\n",
    "It isn't really necissary that you do each of these, but you should be able to adapt your solution to accomodate these changes if you needed to, without it being totally confusing. Each change is just a reshaping of the data going into and out of our model. The multi-dimensional shape of the data isn't the most intuitive thing in the world, so it it totally fine if your mind doesn't immediately manipulate the data in your head. If you diagram out the dimensions, it should be doable though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_LENGTH = 24\n",
    "LOOKBACK = 24*7\n",
    "LEN = 6\n",
    "EXERCISE_EPOCHS = 5\n",
    "EXERCISE_BATCH = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapseHours(dataF, date_col=\"Date Time\", spacing=LEN):\n",
    "    dataF[\"to_keep\"] = dataF.index % spacing == 0\n",
    "    tmp_df = dataF[dataF[\"to_keep\"]]\n",
    "    tmp_df.drop(columns=[\"to_keep\"], inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    collapsed = collapseHours(weather_dataframe, date_col=\"Date Time\", spacing=LEN)\n",
    "    collapsed.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to construct datsets for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_weather_data(dataF, cols, y_col=\"T (degC)\", lookback=720, lookahead=144):\n",
    "    # Select the desired columns\n",
    "    scaler = MinMaxScaler()\n",
    "    data = dataF[cols]\n",
    "\n",
    "    # Prepare the data for an LSTM model\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(lookback, len(data)-lookahead):\n",
    "        x.append(data.iloc[i-lookback:i].values)\n",
    "        y.append(data[y_col][i:i+lookahead].values)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Reshape the data for the LSTM model\n",
    "    x = np.reshape(x, (x.shape[0], x.shape[1], x.shape[2]))\n",
    "    y = np.reshape(y, (y.shape[0], y.shape[1], 1))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only One Feature\n",
    "\n",
    "Try with a strightforward model that only uses one feature, and see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69900, 168, 2) (69900, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "X_one, y_one = prepare_weather_data(collapsed, cols=[\"T (degC)\", \"wd (deg)\"], y_col=\"T (degC)\", lookback=LOOKBACK, lookahead=PREDICTION_LENGTH)\n",
    "del weather_dataframe\n",
    "print(X_one.shape, y_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 09:57:19.595016: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4369/4369 [==============================] - 114s 26ms/step - loss: 56.3019 - mse: 56.3019\n",
      "Epoch 2/5\n",
      "2908/4369 [==================>...........] - ETA: 38s - loss: 18.2187 - mse: 18.2187"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(PREDICTION_LENGTH))\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEXERCISE_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEXERCISE_BATCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m plot_loss(history)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=layer_size, return_sequences=True, input_shape=(LOOKBACK, X_one.shape[2]), dropout=0.2))\n",
    "model.add(LSTM(units=layer_size, dropout=0.2))\n",
    "model.add(Dense(PREDICTION_LENGTH))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n",
    "history = model.fit(X_one, y_one, epochs=EXERCISE_EPOCHS, batch_size=EXERCISE_BATCH, verbose=1)\n",
    "del X_one\n",
    "del y_one\n",
    "plot_loss(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Multi-Feature Model\n",
    "\n",
    "Now that our data is prepared, we can model and predict. Note the input and output shapes in our model:\n",
    "<ul>\n",
    "<li> Input shape - the size of one batch: (# of time steps to look back) * (# of features)\n",
    "<li> Output shape - the size of the output layer: (# of time steps to predict)\n",
    "</ul>\n",
    "\n",
    "No matter how the data was processed above, it should fit this format for the LSTM model. \n",
    "\n",
    "<b>Note:</b> if the lookback and the prediction are large numbers, we are possibly going to run into performance issues in terms of either processing time or memory usage. Try turning down the lookback to 7 days instead of 30 to reduce memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_columns = collapsed.columns\n",
    "print(\"Columns Used:\", weather_columns)\n",
    "X_weather, y_weather = prepare_weather_data(collapsed, cols=weather_columns, y_col=\"T (degC)\", lookback=LOOKBACK, lookahead=PREDICTION_LENGTH)\n",
    "del collapsed\n",
    "\n",
    "print(X_weather.shape, y_weather.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=layer_size, return_sequences=True, input_shape=(LOOKBACK, X_weather.shape[2]), dropout=0.2))\n",
    "model.add(LSTM(units=layer_size, dropout=0.2))\n",
    "model.add(Dense(PREDICTION_LENGTH))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n",
    "history = model.fit(X_weather, y_weather, epochs=EXERCISE_EPOCHS*2, batch_size=EXERCISE_BATCH, verbose=1)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigger Model\n",
    "\n",
    "We can try a larger model to see if we can get better results. This problem is much larger when we use a large number of features, so having a higher capacity model is likely to be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "# fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=layer_size*2, return_sequences=True, input_shape=(LOOKBACK, len(weather_columns)), dropout=0.15))\n",
    "model.add(LSTM(units=layer_size*2, dropout=0.15))\n",
    "model.add(Dense(layer_size*3, activation='relu', kernel_regularizer=keras.regularizers.l2(0.1)))\n",
    "model.add(Dense(PREDICTION_LENGTH))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=metrics)\n",
    "history = model.fit(X_weather, y_weather, epochs=EXERCISE_EPOCHS*10, batch_size=EXERCISE_BATCH*10, verbose=1, callbacks=[early_stopper])\n",
    "plot_loss(history)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
