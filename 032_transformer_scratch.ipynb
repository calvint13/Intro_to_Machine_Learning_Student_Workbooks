{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhackGPT\n",
    "\n",
    "We can make a transformer based model to generate chatGPT-ish text responses. Ours will be far more stupid, but hey, it's a taking computer. Transformer based models are the current state of the art for natural language processing, and most of the models that you've heard of, like ChatGPT, are transformer based models.\n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "What the heck is a transformer, what does it do, and why is it so cool? A transformer model is a type of neural network that was creating in 2017 at Google. The core idea behind transformers is the idea of attention, which is deailed a little bit below. The diagramed strucutre of a transformer model can be a little intimidating, but we can make sense of the critical parts without too much issue. We will mostly sidestep this large diagram, and focus on a basic transformer model that is a little easier to understand.\n",
    "\n",
    "![Transformer](images/transformer.png \"Transformer\")\n",
    "\n",
    "A transformer model contains a few key parts, each of with is dealt with in more detail below.\n",
    "<ul>\n",
    "<li> Embedding - the embedding layer generates embeddings (vector representations) for each token. The embeddings are created for both the token itself and its position in the sequence. </li>\n",
    "<li> Attention layers - the attention layers are the core of the transformer model. They are responsible for creating a representation of the input sequence that is used to generate the output sequence. </li>\n",
    "</ul>\n",
    "\n",
    "The attention part is the star of the show, it is a method to be able to focus the attention of the model on the critical portions of the input sequence and generate contextually informed predictions for the output. As well, transformers do all of this in a way that is more parallelizable than LSTM based models that were the state of the art before transformers, only a few years ago. \n",
    "\n",
    "This image comes from Google, like transformers, and is a great visual representation of how attention works. This example is from a translation model, but a more simple version applies to what we are about to do. The key thing to note is that the image shows:\n",
    "<ul>\n",
    "<li> Several encoding layers, each of which generates a representation of each word in the input sequence by looking at all of the others. </li>\n",
    "<li> Several decoding layers, which generate a representation of the output sequence by looking at all of the words in the input sequence, as well as the current sequence being generated. </li>\n",
    "</ul>\n",
    "\n",
    "This highlights the key idea of attention, which is that the model can look at any part of the input sequence to understand the current word in the input, as well as generate the current word of the output. This differs from the LSTM models that are tied to understanding the data only as a sequence, and not as a whole. The transformer model learns to look at, or pay attention to, the important parts of the input, irrespective of their position in the sequence. This is important in language, as we can have words that impact the meaning of other words at any place in the sequence. Think of an example sentence, \"he isn't the largest, fastest, strongest, or tallest, but the walk-on scrapper Rudy is the heart of the team\". In this sentence \"he\" applies to Rudy, as does \"scrappy\" and \"heart of the team\", which is clear in English, but maybe not so easy for a machine. The transformer model is able to look at the sentence, and work to learn the relationships between the words, and the context in which they are used - so if we were generating a similar sentence, the model knows that after \"scrapper\" we need a person, a \"he\", more specifically.  \n",
    "\n",
    "<b>Important Note:</b> these generative text models seem smart, and in some senses they are, but in the most critical sense, they really aren't, and can be confidently and totally wrong. The model understands what is written and how to construct blocks of text; the model does not understand the underlying meaning. If you ask ChatGPT what it's like to bite an apple, you'll probably get descriptors like \"sweet\", \"tart\", and \"juicy\", which are all accurate. The model doesn't know what it is like to bite an apple, it just knows that if it needs to supply descriptors of an object \"apple\", sweet, juicy, and tart are ones that commonly come up. If it was trained on text written by some apple-hating lunatic, who wrote about how mushy, bitter, and gross apples are, that's what the model will confidently state an apple is like. Even when models learn to do chemistry or math, they are only learning to replicate what they have seen - they don't understand the underlying concepts. With subject like those, there tends to be more strictly defined rules than with the English language, which is pretty loose, so it isn't surprising that these models are quick to learn those subject areas, even if it seems difficult and almost impossibly fast for us as humans - a model can learn a linear regression, from examples only, very quickly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The embedding here has two parts:\n",
    "<ul>\n",
    "<li> Token embedding: This maps each token to a vector representation in N-dimensional space. This is what we are used to for embedding. The original transformer paper used a 512-dimensional embedding, so each token was represented by a vector of 512 values that position it on a 512D grid. \n",
    "<li> Positional embedding: This maps each token's position in the <i>sequence</i>. The position embedding can be thought of as an extension of the concept of just tracking which word of a sentence each token is, 1,2,3...\n",
    "</ul>\n",
    "\n",
    "#### Token Embedding\n",
    "\n",
    "Token embedding is something that we are used to from when we used word2vec to generate embeddings for classification models. We are tranlating each token into an N-dimensional representation in space. The big difference here is that our embedding space is being learned by the model during training, so we should expect that the model will be shifting each token around in space as it learns more about what that word means, or more accurately, how it is used in our training data. \n",
    "\n",
    "![Embedding](images/embedding.png \"Embedding\")\n",
    "\n",
    "#### Positional Embedding\n",
    "\n",
    "The positional embedding is needed and most clearly seen if we compare this to an LSTM. In an LSTM, the position of a token is always known as we process the data sequentially. In the transformer model, the data is taken in parallel, so we don't have the sequence data built in. This has the benefit of allowing the model to process more of its work in parallel than an LSTM, but it also means that the model needs to be told where each token is in the sequence. What is the positional embedding? It follows the same concept as the token embedding, we are representing something with a vector of values. In the positional embedding, the math is a little involved, but it uses sine and cosine functions to represent the position of a token. \n",
    "\n",
    "![Positional Embedding](images/positional_emb.png \"Positional Embedding\")\n",
    "\n",
    "Where:\n",
    "<ul>\n",
    "<li> <b>k:</b> position of the token. \n",
    "<li> <b>d:</b> dimension of the embedding.\n",
    "<li> <b>i:</b> used for mapping to both sine and cosine functions.\n",
    "</ul>\n",
    "\n",
    "This positional embedding uses the trig functions to introduce some additional capability to our embedding values. First, this helps if we encounter longer sentences later on - if we embedded the position with a simple word count number, that would be an issue for us. Second, the trig functions allow us to embed the position in a way that is not deterministic. This means that the model can learn where tokens occur in relation to each other without being told explicitly. This is useful if you think of sentences such as:\n",
    "<ul>\n",
    "<li> I do not like the story of the movie, but I do like the cast.\n",
    "<li> I do like the story of the movie, but I do not like the cast.\n",
    "</ul>\n",
    "\n",
    "These two sentences use the same words, but the meaning is opposite. The positional embedding helps capture the relationship between the words based on where the occur, and connect words that occur in certain \"areas\" to those in other \"areas\" of a sentence. This is really useful if you think of something like an adjective, that adjective modifies some noun, and understanding English requires that we are able to identify which noun it belongs to. Positional embedding with sine/cosine help with that, the position is recorded not only in a way that tells us where a word sits in an absolute sense, but it tells us where that word sits relative to the other words it is with. This is one reason transformers are so useful for tasks like language, their ability to contextualize the relationships in parts of text surpasses that of other models that we have today; when generating text, this gives us the most natural sounding text, as the \"next word\" prediction is based on a more comprehensive understanding of the sentence. \n",
    "\n",
    "Notably, the positional embedding uses the word embedding dimension, d, as the dimension of the positional embedding. This is because the positional embedding is added to the token embedding, so the two need to be the same dimension. This means that the embedding matrix generated can be quite large for each token. This also means that the input to any future modelling is going to contain those two vectors, likely represented in a high dimension - what is the token, and where is it in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Construction\n",
    "\n",
    "We can now create a function to construct the core piece of our model, the transformer. The transformer layer has a few parts, the critical one being the attention layer. \n",
    "\n",
    "<b>Note:</b> the declarations of the layers are slightly different in a functional model. Each layer is a function that takes an input tensor and returns an output tensor. The layers are then called in the call method of the model.\n",
    "\n",
    "### Pay Attention\n",
    "\n",
    "The core piece of the transformer architecture is the attention mechanism. Attention serves as a way to focus, or pay attention, to certain parts of the input. The original paper that outlined the transformer architecture, \"Attention is All You Need\" from 2017 outlined the concept of attention at a high level - the idea is that we can use attention to focus on the most important parts of the input sequence, and use that to generate the output sequence. This is what humans do when reading something, we understand that certain parts of text add context allowing us to understand other parts. Attention is a way to teach a model to do the same thing. In RNN models, the data is always processed as a sequence - this has the advantage of allowing us to always know the order, but it slows computation and prevents the model from excelling at building relationships between parts of the data that are far from each other in the sequence. The transformer architecture replaces sequential processing of a sequence of values with parallel processing of the entire sequence at once, and build connections between the current part we are working on (e.g. the next word to generate) and all other parts of the sequence, focusing our attention on the most important parts.\n",
    "\n",
    "![Attention](images/attention.png \"Attention\")\n",
    "\n",
    "The result of that can be illustrated in the diagram above, with some additional labels. Here we have an illustration of a sentence, focusing on the last word, with colored and shaded lines indicating how much \"attention\" that word (token) needs to pay to the other tokens. Phrased differently, the stronger the line, the more that token on the left influences what the last token on the right should be. With training and lots of data, the model can eventually learn which other parts of a sequence the current token should \"pay attention to\", and use that to generate the next token in the sequence. Given enough training, this goes beyond words, and the model will begin to understand the relationships between types of words and parts of sentences. As a noticable example, words such \"it\", \"in\", or \"is\" can be used in many different contexts, and often refer to different nouns or verbs in a setence. The models can begin to decipher that certain types of words come after \"is\", and others come after \"in\". In this example, the model is paying strong attention to \"in\" and \"European\", since the model has learned enough that \"in\" indicates that the next word is some variety of place, and \"European\" indicates that said place is likely somewhere in Europe (the self-attention can be ignored). As models learn, see more language, and see sequences that are structured differently, the model will begin to understand more about the relationships between words and parts of sentences, and use that to generate sentences that not only use the correct words, but are structured in a way that mirrors the training data that it has seen. This sometimes gives the appearance that the model is thinking up and generating these answers on the fly, based on what it knows, but the language models don't really \"know\" anything, in an epistemological sense. Generative language models are just excellent at subtle pattern recognition in language, and reproducing an output that follows what it expects. The massive capacity and volume of training data is what gives it the ability to draw on patterns seen is \"the internet\" worth of text, thus allowing it to generate text that can avoid the robotic feel that small models have. \n",
    "\n",
    "#### Attention Implementation\n",
    "\n",
    "The attention mechanism contains three key matrices that we'll ultimately use to calculate things:\n",
    "<ul>\n",
    "<li> Query\n",
    "<li> Key\n",
    "<li> Value\n",
    "</ul>\n",
    "\n",
    "The query, key, values are commonly described as analagous to doing a Google search. For example, when you search for videos on Youtube, the search engine will map your <b>query</b> (text in the search bar) against a set of <b>keys</b> (video title, description, etc.) associated with candidate videos in their database, then present you the best matched <b>values</b> (videos).  \n",
    "\n",
    "Using the query, key, and value objects involves a multistep process. \n",
    "<ul>\n",
    "<li> First, the query, key, and value all get a copy of the embedding (position and token) matrix fed in, which is then multipled by a set of weights that belong to a linear layer (no activation) for that Q/K/V input. \n",
    "<li> The value matrix is set aside for the moment. \n",
    "<li> The results of the query and key matricies are then mutipled by each other, which generates attention scores. \n",
    "<li> The result is then passed through a softmax function to normalize the weights and generates the actual attention mask. \n",
    "<li> The normalized weights are then multiplied by the value matrix, which gives us the final output.\n",
    "</ul>\n",
    "\n",
    "To ultimately create the layer, we have several of these heads, similar to filters in a CNN. \n",
    "\n",
    "![Multi-Head Attention](images/multi_head_att.png \"Multi-Head Attention\")\n",
    "\n",
    "Take and example of a sentence being, \"“Anthony Hopkins admired Michael Bay as a great director\", the product of the query and key matricies would look something like this:\n",
    "\n",
    "![Attention Mechanism](images/key_value.png \"Attention Mechanism\")\n",
    "\n",
    "These attention scores are measures of how important each word in the input sequence is to each other word. We normally see each word being really important to itself, then as the similarity decreases, the importance decreases. In this example, \"Hopkins\" and \"Anthony\" have a high score of attention with respect to each other, which makes sense! We would likely want to produce those two words in sequence. Given large amounts of data, the model can become very good at identifying what is important and what is not, and in particular, understanding context. Because the attention is based on the positon and token embeddings, and we have multiple heads (see below) each honing in on some other aspect of the text, the model can learn relationships between parts of speech that are challenging for other types of models, such as a sentence that has a lage independent clause in the middle of it or figures of speach that have little impact on the meaning of a sentence. Importantly, each token in a sentence is taken as the input, so we generate such a matrix for each \"query\" token.\n",
    "\n",
    "![Attention Sequences](images/attention_seq.png \"Attention Sequences\")\n",
    "\n",
    "#### Attention Masking\n",
    "\n",
    "Once we get the attention mask, we combine it with the value matrix to get the final output from our attention layer. The easiest way to think of applying an attention mask is with an example from computer vision. The \"thing\" that we are trying to do with computer vision, say image recognition, is to capture information from the \"important part\" of the image. We don't want to focus, normally, on background stuff. The attention mask serves to act basically as a filter, that blocks out the less important and lets through the more important. So we can think of the end result as the input + mask = useful output. This image is a little blurry, but it shows the idea. If we have a model being trained to identify objects, we might end up with a mask that looks like this. Note the final result and the original (which has had the color space changed). The desired result is the bottom left, where the objects we want to identify are the focus. Applying the mask to the original serves to do that - remove the less important stuff, emphasize the more important stuff. With language, we get the same thing. We want to focus on the important parts of a sentence and ignore the less important parts - that measure of importance is what we are learning during training. \n",
    "\n",
    "![Attention Mask](images/attention_mask.png \"Attention Mask\")\n",
    "\n",
    "<b>Note:</b> we also have a causal mask, which is used to prevent the model from \"cheating\" by looking ahead in the input sequence. This effectively stops the model from just looking up the answer, which would let it sidestep learning. \n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "The layer that we are adding is called a multi-head attention layer, implying that we have multiple attention filters at once. This part works similarly to how the convolutional filters work in a CNN. Each filter in a CNN learns to identify some useful feature in that context - edges, colors, etc... Here, each attention head learns to focus on a different aspect of the input, language in our case. As our model is trained, each attention head will learn to focus on different aspects of the input. Recall that the weights for the filter are normally random initially, so the training process will cause each one to find its own thing to focus on as we shrink the loss. \n",
    "\n",
    "### Attention Magic\n",
    "\n",
    "This is a very brief and high level overview of attention and its application to our neural networks. There is a lot more to it, it is a very interesting topic, and based on what we know now (2023), transformer based models will likely be exceedingly common over the near future. If you want to learn more, I recommend the following resources:\n",
    "<ul>\n",
    "<li> https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/\n",
    "<li> https://www.youtube.com/watch?v=6D4EWKJgNn0\n",
    "<li> https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections-padding-masks-all-the-details-of-transformer-model/\n",
    "</ul>\n",
    "\n",
    "The ability of the transformer models to, without external direction, learn what is important and what is not is what makes them both so powerful and so flexible. The examples of the GPT models accurately performing tasks that it wasn't trained on are good examples of this flexibility. If we have training data to supply the transformer model, it can very accurately learn to extract what matters from what doesn't, irrespective of the specific task that it is working on, which makes learning that task much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "We can now create the model, and we will use the portions that we constructed above. The basic parts are:\n",
    "<ul>\n",
    "<li> Token and positional embedding - create representations of each sequence. \n",
    "<li> Transformer layers - the core of the model.\n",
    "<li> Output layer - dense layers to convert the output of the transformer layers to the output of the model.\n",
    "</ul>\n",
    "\n",
    "The basic structure of different varieties of neural networks is also seen here, we again have a dense neural network to generate predictions from inputs, and that network can be fed by either:\n",
    "<ul>\n",
    "<li> Our actual data, for normal regression or classification.\n",
    "<li> The output of convolutional layers, for image processing. \n",
    "<li> The output of recurrent layers, for sequential data.\n",
    "<li> The output of transformer layers, for quickly expanding types of tasks. \n",
    "</ul>\n",
    "\n",
    "No matter the specific implementation, the basic structure, and ability to learn, is the same in all neural networks. The ability to learn relationships that are complex, obscure, and impossible for a human to describe makes neural networks extremely powerful. If we can generate some architecture that is good at extracting features from some specific type of data, we can combine that with a regular neural network to make all kinds of predictions or generate new data. Our \"predictor\" dense model, and the \"extractor\" early layers can then both learn epoch by epoch, together, to be as accurate as possible. As the capacity of processors increases and the experience of researchers grows, we can expect to see more and more expansion in what neural networks can do. In particular, the increased ability to parallelize the processing of sequential data with the transformer architecture is massively helpful - we saw in the LSTM models the depth of the sequences of calculations meant that growing models to be very powerful requires lots of processing, in a way that is extremely hard to parallelize, limiting the growth. Transformers can do more in parallel, and it is much easier to add another processor than it is to develop a processor that is twice as fast; these models will likely grow to more efficiently process data accross large networks of worker machines, generating larger and more powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_RESOURCE = False\n",
    "\n",
    "vocab_size = 10000  # Only consider the top X words\n",
    "maxlen = 40  # Max sequence size\n",
    "embed_dim = 196  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 196  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "batch_size = 512\n",
    "EPOCHS = 10\n",
    "\n",
    "LOAD_WEIGHTS = True\n",
    "\n",
    "if HIGH_RESOURCE:\n",
    "    vocab_size = 20000  # Only consider the top 20k words\n",
    "    embed_dim = 512  # Embedding size for each token\n",
    "    num_heads = 8  # Number of attention heads\n",
    "    feed_forward_dim = 512  # Hidden layer size in feed forward network inside transformer\n",
    "    batch_size = 1024\n",
    "    EPOCHS = 100\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None], metrics=[\"accuracy\", None]\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Load Weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data and Prepare for Training\n",
    "\n",
    "This example uses some movie reviews for source data. The dataset comes already split into positive and negative labels, for classification, and into training and testing sets. We don't need any of these divisions, we just need all the text for training, so the data preparation steps here are:\n",
    "<ul>\n",
    "<li> Download the data.\n",
    "<li> Loop through all the files and generate a list of all the file names. \n",
    "<li> Crate a dataset from all the files. \n",
    "<li> Clean the data by removing the html tags and punctuation.\n",
    "<li> Tokenize the data by splitting the text into words and creating a vocabulary.\n",
    "<li> Create training ready data by creating sequences of X = \"up to the current word\" and Y = \"the next word\".\n",
    "<li> Set the dataset to be shuffled, batched, and prefetched.\n",
    "</ul>\n",
    "\n",
    "<b>Note:</b> there are a few odd [UNK] tokens, this is a placeholder for words that are not in the vocabulary. Were this a production model, we'd want to come up with some more sophisticated way of handling this, but for this example, we'll just leave it as is. When dealing with natural text, it is common to have things like this for unknown data, or other special tokens for the beginning or end of a sentence (e.g. [BOS] or [EOS]).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  5168k      0  0:00:15  0:00:15 --:--:-- 7275k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 files\n",
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 24.00 GB\n",
      "maxCacheSize: 8.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 19:10:53.552262: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf.data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at one example of the data below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 40) (512, 40)\n",
      "Tokens: [ 670 3908    9   66  526  329  172    3   12  213  987   14   12   32\n",
      "  516    2  115   14   29  650    1   83    1   27    4   53   69   20\n",
      "  748   76    2 1713    7  324    5 2165  707    6  323    1] \n",
      "\n",
      " [3908    9   66  526  329  172    3   12  213  987   14   12   32  516\n",
      "    2  115   14   29  650    1   83    1   27    4   53   69   20  748\n",
      "   76    2 1713    7  324    5 2165  707    6  323    1    1]\n",
      "Sentence: robert altman is my favorite american director . i must admit that i have enjoyed the films that are usually [UNK] : [UNK] \" , if only for giving me the pleasure of seeing a grown -up and beautiful [UNK]  \n",
      "\n",
      "Next word: [UNK]\n"
     ]
    }
   ],
   "source": [
    "tmp = text_ds.as_numpy_iterator()\n",
    "x_tmp, y_tmp = next(tmp)\n",
    "print(x_tmp.shape, y_tmp.shape)\n",
    "samp_x = x_tmp[0]\n",
    "samp_y = y_tmp[0]\n",
    "print(\"Tokens:\", samp_x, \"\\n\\n\", samp_y)\n",
    "word = \"\"\n",
    "for x_ in samp_x:\n",
    "    word += vocab[x_] + \" \"\n",
    "print(\"Sentence:\", word, \"\\n\\nNext word:\", vocab[samp_y[-1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback\n",
    "\n",
    "To get our text out, we can use a callback that will be called at the end of each epoch. We can still get things from \"predict\" after the fact, but this will give us some step by step evidence of our program's smarts. We will make two instances of this callback, each with different seeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1, log_dir=\"logs\"):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Run, Predict\n",
    "\n",
    "Now that the model is created, we can fit it to the training data then test out the abilities. Our prediction is an incremental process, we start with a seed, then we predict the next word, then we add that word to the seed, and predict the next word, and so on. At each step, the model looks at the input to this point, calculates the attention, finds the most suitable (highest score) word from the vocabulary, generates it, calculates the loss (N-dimensional \"closeness\" from embedding), and moves one more step forward. This training process can take a very long time, loss kept slowly improving for me for 100+ epochs without fully flattening. With larger models and datasets, this pattern will likely still occur, only at a far larger number of epochs. This is a very tangible example of the need for fast machines to turn around quick training cycles, look at the quality of the sample lines from epoch 1 to epoch 20+, the difference is often striking. Here is one example that I took from in the middle of training, at epoch 38:\n",
    "<ul>\n",
    "<li> <b>Epoch 1:</b> this movie is [UNK] to of . to of . , to the . a the a the , , . . the . the [UNK] the , the , [UNK] the of a the the . . and . is [UNK] to [UNK]\n",
    "<li> <b>Epoch 38:</b> this movie is not just a bad movie . the plot is outrageous and unbelievable . sure the characters are unbelievable and the ending will be a better surprise , but the ending was just sad .       \n",
    "</ul>\n",
    "Not a bad improvement. Like most models, we normally see a really quick improvement in the first few epochs, to get to \"tolerable\", we then usually get a really slow improvement as the model gets slightly better each epoch, for a long time. Here I am only looking at the loss, which isn't really a number that has contextual meaning on its own, but it gives us a metric of how well the model is doing in comparison to itself. The accuracy metric isn't really useful here as we want to generate \"a\" correct word, not specifically \"the\" correct word, so generating \"vehicle\" instead of \"automobile\" is still a good answer, as we'd expect them to be quite close in the N-dimensional embedding. There are other metrics that are commonly used to evaluate text generation, such as BLEU and Rouge, that aim to generate some form of an \"accuracy\" score for the text that is created for a model. Any accuracy measure when we are creating something for human consumption is only a guideline - our model can optimize for loss, but loss is only a proxy for \"the model thinks of a good next word\" - there isn't any way to directly calculate how close to \"real speach\" our generated data is. We won't get into those other metrics here, the loss and a subjective evaluation of the text is good enough for us. As large language models and generative models become more and more prevelant, there are constantly new metrics being developed to provide a benchmark for the quality of those models. All of these metrics are limited in their usefulness, as there really isn't a single RMSE-like value that makes sense for the quality of a generated block of text - the scores will broadly correlate with \"model quality\", but those scores won't be as meaningful as a \"real\" accuracy measure like RMSE, accuracy, or F1. These measures are more of a rough indication than a strict measurement. \n",
    "\n",
    "<b>Note:</b> Trying to train this on my laptop on CPU took forever, I didn't get to the point where the first epoch gave me a time estimate. Using the accelerated M2 Mac is faster, and using Google Colab or a GPU it is much, much faster. If you are running this it is a very good idea to use some variety of GPU for practicality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt1 = \"this movie is\"\n",
    "start_tokens1 = [word_to_index.get(_, 1) for _ in start_prompt1.split()]\n",
    "start_prompt2 = \"Skiing fast makes me\"\n",
    "start_tokens2 = [word_to_index.get(_, 1) for _ in start_prompt2.split()]\n",
    "num_tokens_generated = 40\n",
    "\n",
    "log_dir = \"logs\"\n",
    "log_1 = str(log_dir + \"/1\")\n",
    "log_2 = str(log_dir + \"/2\")\n",
    "weight_path = \"weights\"\n",
    "text_gen_callback1 = TextGenerator(num_tokens_generated, start_tokens1, vocab, log_dir=log_1)\n",
    "text_gen_callback2 = TextGenerator(num_tokens_generated, start_tokens2, vocab, log_dir=log_2)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(weight_path, save_weights_only=True, monitor=\"loss\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "if LOAD_WEIGHTS:\n",
    "    weights_url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/lstm_gen_weights(1).keras\"\n",
    "    old_weights = keras.utils.get_file('cust_transform_weights.keras', weights_url)\n",
    "    model.load_weights(old_weights)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/stepense_2_loss: 6.2811 - dense_2_accuracy: 0.0805 - transformer_block_accuracy: 0.00\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 21s 21s/step\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "generated text:\n",
      "this movie is a good time to make [UNK] of all , [UNK] [UNK] [UNK] [UNK] [UNK] . you have a [UNK] . a great . it was an [UNK] to me . it was a few to me to the make make see\n",
      "\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "generated text:\n",
      "[UNK] fast makes me [UNK] [UNK] , and the film of this . [UNK] , a good [UNK] of the [UNK] , and the [UNK] . i have been a good . but i can see the acting is the acting most film movie movie\n",
      "\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "checkpoint.tmp3627edaa72324296af83ee31915aa24c; Is a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m create_model()\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(text_ds, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, epochs\u001b[39m=\u001b[39;49mEPOCHS, callbacks\u001b[39m=\u001b[39;49m[text_gen_callback1, text_gen_callback2, checkpoint_callback])\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py:622\u001b[0m, in \u001b[0;36mrename_v2\u001b[0;34m(src, dst, overwrite)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mio.gfile.rename\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrename_v2\u001b[39m(src, dst, overwrite\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    611\u001b[0m   \u001b[39m\"\"\"Rename or move a file / directory.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \n\u001b[1;32m    613\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m   _pywrap_file_io\u001b[39m.\u001b[39;49mRenameFile(\n\u001b[1;32m    623\u001b[0m       compat\u001b[39m.\u001b[39;49mpath_to_bytes(src), compat\u001b[39m.\u001b[39;49mpath_to_bytes(dst), overwrite)\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: checkpoint.tmp3627edaa72324296af83ee31915aa24c; Is a directory"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(text_ds, verbose=0, epochs=EPOCHS, callbacks=[text_gen_callback1, text_gen_callback2, checkpoint_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We can now create some predicted text with our trained model. Below are just some helper functions to make predictions of a certain length. The quality of the text is highly, highly variable - the dataset used was only one source, and wasn't massive. The model is also pretty small. I ran this once for 100 epochs of training, and the results I got for generated text were:\n",
    "<ul>\n",
    "<li> this movie is not really a bad movie . there are only two stars (the heroine ) almost all vampire tales being [UNK] i psycho if your life . it wasn 't that part of this movie . i wanted to see more like that , but i just have to say i really liked it that movie and even the music was . in my opinion the movie itself and it really gets a look like something that you might can just really can just a bad movie . there are only two stars (the heroine ) almost all vampire tales being [UNK] i psycho if your life . it wasn 't that part of this movie . i wanted to see more like that , but i just have to say i really liked it that movie and even the music was . in my opinion the movie itself and it really gets a look like something that you might can just really can just \n",
    "<li> fast makes me forget . check . when you think carl brashear [UNK] gooding jr . and his navy master chief diver bill finds himself inside his hands , and even by his face running his navy , deep and surface story of carl brashear 's brother , played by cuba gooding jr . he died in his obsession with his cat competition . . the physical comedy gives him great example of his first powerful karate and kung fu and and master . forget . check . when you think carl brashear [UNK] gooding jr . and his navy master chief diver bill finds himself inside his hands , and even by his face running his navy , deep and surface story of carl brashear 's brother , played by cuba gooding jr . he died in his obsession with his cat competition . . the physical comedy gives him great example of his first powerful karate and kung fu and and master . \n",
    "<li> are going to make this country great movie on which you think the best film ever ? i am from a big city photographer . but then i saw \"one dark night [UNK] summer \" , and i left the store , the week later when i hit rock channels . i see 15 [UNK] only two films that have noted a very strange mixture of suspense and interaction between nature and noble and i found the kind of movies dimension . . . . . . . movie on which you think the best film ever ? i am from a big city photographer . but then i saw \"one dark night [UNK] summer \" , and i left the store , the week later when i hit rock channels . i see 15 [UNK] only two films that have noted a very strange mixture of suspense and interaction between nature and noble and i found the kind of movies dimension . . . . . . . \n",
    "<li> my dogs at that point i can 't say i am impressed with this movie for the first 10 years but i don 't remember any things i liked were in this animated movie , but there were something good in it . the animation is narration from the music , the songs were played : \"you sons ' \" and family how a kid died until after 9 years ! \" . it was not that bad it was done bad done used that point i can 't say i am impressed with this movie for the first 10 years but i don 't remember any things i liked were in this animated movie , but there were something good in it . the animation is narration from the music , the songs were played : \"you sons ' \" and family how a kid died until after 9 years ! \" . it was not that bad it was done bad done used\n",
    "</ul>\n",
    "\n",
    "Not amazing, but not terrible either. We have results that are more or less sentences, with some parts where we really go off the rails. For a small model, short training, and tiny dataset, I'd say we are doing reasonably well. Below, the sample_k value is somewhat similar to our Temperature parameter. This controls the amount of randomness in the prediction by controling the number of values that we are choosing from when generating a word. This k represents the number of most-likely words we are choosing from, so if it is 5, our generated word will come from the top 5 most likely words. If it is 1, we will only choose from the most likely word. This is called top_k sampling, it is a pretty simple method to add randomness to a generative model. As with any model, as we move farther out, we get more and more weird results - that really requires a larger model to capture well. There are several different ways that this selection process can be done, with each method applying a different algorithm to add randomness to the final generated token, this link is a good overview of some of the different choices at a high level: https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Weights\n",
    "weight_save_path = \"custom_transformer_weights.keras\"\n",
    "model.save_weights(weight_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some simple logic to, at least partially, de-UNK it\n",
    "unknowns = [word_to_index.get(_, 1) for _ in [\"<unk>\", \"<UNK>\", \"<Unk>\", \"<uNk>\", \"<unK>\", \"<UnK>\", \"<uNK>\", \"<UNk>\"]]\n",
    "\n",
    "def indToSentence(ind, dict):\n",
    "    word = \"\"\n",
    "    for n_ in ind:\n",
    "        word += dict[n_] + \" \"\n",
    "    return word\n",
    "\n",
    "def sentenceToInd(sentence, dict):\n",
    "    indicies = []\n",
    "    words = sentence.split()\n",
    "    for word in words:\n",
    "        if isinstance(word, str):\n",
    "            index = dict.get(word)\n",
    "            if index is not None:\n",
    "                indicies.append(index)\n",
    "    return indicies\n",
    "\n",
    "def sample_from(logits, sample_k = 2):\n",
    "    logits, indices = tf.math.top_k(logits, k=sample_k, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    choice = np.random.choice(sample_k, p=preds)\n",
    "    #if choice in unknowns:\n",
    "    #    choice = np.random.choice(sample_k, p=preds)\n",
    "    return choice\n",
    "\n",
    "def generateText(model, index_to_word, word_to_index, startPrompt, length=80, sample_k = 12):\n",
    "    start_tokens = sentenceToInd(startPrompt, word_to_index)\n",
    "    num_tokens_generated = 0\n",
    "    tokens_generated = []\n",
    "    while num_tokens_generated <= length:\n",
    "        pad_len = maxlen - len(start_tokens)\n",
    "        sample_index = len(start_tokens) - 1\n",
    "        if pad_len < 0:\n",
    "            x = start_tokens[:maxlen]\n",
    "            sample_index = maxlen - 1\n",
    "        elif pad_len > 0:\n",
    "            x = start_tokens + [0] * pad_len\n",
    "        else:\n",
    "            x = start_tokens\n",
    "        x = np.array([x])\n",
    "        y, _ = model.predict(x)\n",
    "        #sample_token = np.argmax(y[0][sample_index])\n",
    "        logits = y[0][sample_index]\n",
    "        sample_token = sample_from(logits, sample_k)\n",
    "        tokens_generated.append(sample_token)\n",
    "        start_tokens.append(sample_token)\n",
    "        num_tokens_generated = len(tokens_generated)\n",
    "    txt = indToSentence(start_tokens + tokens_generated, index_to_word)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size = 15\n",
    "\n",
    "t1 = generateText(model, vocab, word_to_index, \"this movie is not really\", sample_k = k_size)\n",
    "t2 = generateText(model, vocab, word_to_index, \"Skiing fast makes me\", sample_k = k_size)\n",
    "t3 = generateText(model, vocab, word_to_index, \"We are going to make this country great\", sample_k = k_size)\n",
    "t4 = generateText(model, vocab, word_to_index, \"Where my dogs at\", sample_k = k_size)\n",
    "t5 = generateText(model, vocab, word_to_index, \"How many licks would it take to\", sample_k = k_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate Generation Results\n",
    "\n",
    "<b>These came from a different execution, where I set the parameters to be resource intensive (layers, heads, etc...)</b>\n",
    "\n",
    "One note from this set of generated data is at the end of the first listing, Christopher Walken. Our model is only based on one-grams, so we don't have two word ngrams in the data. Despite this, the generated results are still able to pair the first and last name, and place it in a spot that is contextually supposed to be an actor. Great work, model! As well, we have something like \"the acting and story is good , although it is quite campy\", a phrase that definitely could be separated into two simple sentences, but is combined to make a more complex and more natural sounding phrase. We also get, \"so you get naked\" and \"the actors are kinda stupid\" as generated phrases, so we are confident that the text we are generating is inline with our source data - internet movie reviews.\n",
    "\n",
    "We can also highlight the repeated spaces and periods. These are tokens that we need, we can't strip them out like we previously would for classification based on meaning. Our results would be inarguably better if we ran the results through some type of grammar/spell checker here. The model will eventually get much better at this if we supplied more data and epochs - punctuation and spaces occur all over the place in actual language, in a nearly unlimited number of variations, so picking up on patterns of what-goes-where requires more data for our naive model to learn. Think of someone adding spaces in an online text box, it might be due to lining up some words on the screen, or cutting/pasting, general laziness, or bad grammar. In any case, we may have any number of spaces, almost anywhere, with no real relation to the underlying meaning of the text. If this was a specific problem we are trying to solve in our model, we could preprocess the data. If we are making a large model to be used in many different applications, we may not want to do this, as it could impact other applications - programming in python is whitespace sensitive, a.k.a. the meaning of the code depends on the number of cases. In general model applications, our model will outlearn things like this, given enough training.\n",
    "\n",
    "<ul>\n",
    "<li> this movie is not really good . it is well done in the acting and story is good , although it is quite campy . and the story is good and the special effects are also excellent . i think the cast is really good , that of course christopher walken . the plot is simple but aside from laughing out loud moments .                       good . it is well done in the acting and story is good , although it is quite campy . and the story is good and the special effects are also excellent . i think the cast is really good , that of course christopher walken . the plot is simple but aside from laughing out loud moments .                       \n",
    "<li> fast makes me think mtv movies and nowadays they usually are funny trash movies like the mindless shows that are somewhat good movies like scream ,but in turn off the whole thing into a mindless sort of thing , is the idea why someone gets upset by some people ) . . . this was a bad movie . . the actors are kinda stupid .                   think mtv movies and nowadays they usually are funny trash movies like the mindless shows that are somewhat good movies like scream ,but in turn off the whole thing into a mindless sort of thing , is the idea why someone gets upset by some people ) . . . this was a bad movie . . the actors are kinda stupid .                   \n",
    "<li> are going to make this country great movie . . .no scenery (some japanese are not known , but it just tells us that he gets to know what happened to them , so you get naked , but then this time with it and a more serious movie . . . but that 's not worth hearing about this telling of a friend was mine that they decided to buy it anyway . i 'm not going to say how , that the what why , the movie . . .no scenery (some japanese are not known , but it just tells us that he gets to know what happened to them , so you get naked , but then this time with it and a more serious movie . . . but that 's not worth hearing about this telling of a friend was mine that they decided to buy it anyway . i 'm not going to say how , that the what why , the \n",
    "<li> my dogs at this [UNK] nasty (very short ) and mario [UNK] crop of other films . (and this is like many of us , talk about dinosaurs heaven ) starts with dinosaurs . mountains of hot [UNK] on a tour of performance which sets the island in dinosaurs and some are back to their craft . they both give us footage of animals to hire some real storyteller . often incoherent films such as don 't get me wrong , , , , this [UNK] nasty (very short ) and mario [UNK] crop of other films . (and this is like many of us , talk about dinosaurs heaven ) starts with dinosaurs . mountains of hot [UNK] on a tour of performance which sets the island in dinosaurs and some are back to their craft . they both give us footage of animals to hire some real storyteller . often incoherent films such as don 't get me wrong , , , , \n",
    "</ul>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Exercise\n",
    "\n",
    "Try to adapt this, or any transformer based text-generation model, to a new set of training data. For the most part, this example could be adapted to new data pretty easily, or work from scratch if you feel comfortable. There are lots of examples online if you Google something like, \"keras transformer generate text\". A recommendation is to try to use source training data that is simple, or written for children. The reason for this isn't really based on the model, but you're likely limited by Colab's free resource limits, and simple text requires a smaller model and less training - the experience is likely just a bit easier. <b>Above all else, generative text models are pretty popular currently, so being able to truthfully say that you made a SpongebobGPT or something from scratch is likely a good resume point and portfolio piece.</b> If nothing else, it'll impress any love interests you find over the summer. \n",
    "\n",
    "### Transformer Wrap-Up and Use\n",
    "\n",
    "Transformers are new, so we don't have all that much experience using them and applying them to a variety of problems. To this point, it looks very likely that transformer based models will be the leaders in sequential data, and potentially much more. The key ability of the transformer to generate \"attention\" between any two values in a sequence, in parallel, is a massive advantage over LSTM models. This advantage lies both in the logic of the model and the ability to more efficiently parallelize the processing, which is hard with sequential data. The ability of transformer models to adapt to many types of problems, with little to no explicit direction, opens the door to many innovations in the near future. Some of the most exciting work is being done in the area of \"few shot learning\", where we can take a model that has been trained on a large amount of data, and then use that model to learn a new task with very little data. As transformer models progress, we'll likely see them edging out other architectures such as RNNs in many applications. \n",
    "\n",
    "The current massively impressive application of transformers is in larger versions of what we are doing, large language models. These models are, at their core, very similar to what we have created, the major difference being:\n",
    "<ul>\n",
    "<li> Far more training text allows the model to use the ability to learn from context to generate a far more accurate understanding of the structure of language. \n",
    "<li> A far larger model allows the model to \"hold\" a large vocabulary and a large number of relationships between words.\n",
    "<li> Many large models utilize some form of human feedback to improve the model, generally by having a human vote for the best sentence generated by the model, or give a thumbs up/down to what a model produces. This adds in some human supervised learning, which can be important in helping the model become more natural sounding. \n",
    "</ul>\n",
    "\n",
    "At the time of writing, the current state-of-the-art in language models is GPT4, which is a much larger evolution from the globally impactful ChatGPT. As computers get better, we should see models that are larger and smarter, especially as the transformer architecture allows us to scale up models by parallelizing the processing. Costs just to train some of these models are counted in the millions of dollars, either from renting cloud resources or purchasing GPUs and paying for electricity, so this is one of relatively few computing problems where computing speed and efficiency are critical limitations. Researchers are working on an assortment of ways to make these training times more efficient, from reducing the precision of calculations inside the neural networks (saving time for each +-*/ operation), to creating more efficient optimization algorithms for gradient descent, to building hardware that is inherently more efficient at training these models. One of the unique things about predictive modelling is that, generally, faster hardware not only means models are trained faster, it means that models can be trained <i>better</i>. Even without any improvements to the code, a model that can try more hyperparameter combinations in a grid search, process more training data, or train for more epochs, will be able to find a better solution. In this sense, faster GPUs and smarter algorithms are two sides of the same performance/accuracy coin; more speed = more attempts = more training = better model.\n",
    "\n",
    "Due to these factors, as well as the increasing number of people with knowledge of neural networks and machine learning, we can expect to see the rate of progress in this area to continue to accelerate, potentially to a shocking extent. Simply adding more data, using newer hardware, and refining existing code will naturally lead to all of these large models getting better and better as time progresses. With many very smart people working on the implementation of the transformers in code, we can also expect to see both general refinements and the potential for some big jumps in ability as we've hinted at. Large language models have really only been a thing that the general public has known about since ChatGPT, so the increased exposure should lead to more brains thinking about it, and more innovation. Since this specific type of model has only existed for ~5 years, there's probably a lot of runway left for incremental improvements - I recently saw a paper on reducing or compressing the number of associations that are saved in the model, as many words in a sentence may not have \"attention\" needed between them; reducing that means fewer weights, less storage, less RAM, and faster processing - assuming you can keep the data you need to make the model! There are also improvements in a model's ability to train itself or generate training data. We looked at augmenting image data, a relatively simple way to add training data to an image model. The ability to create adversarial networks, which generate data then evaluate it as fake or real, also has the potential to speed the development of large neural networks. If we can create feedback that is \"good enough\" for training, we can let models just train indefinately and keep improving; we commonly see this type of approach on models that play games like chess, they play against themselves until they are amazing. A relatively small improvement in any of the underlying constraints, such as the ability to perform the weight update calculations slightly faster, will probably lead to outsized improvements in model performance down the line. \n",
    "\n",
    "This quick and accellerating development of the abilities of predictive models is extremely exciting, but also somewhat worrying. There is generally a low understanding of AI tools in the population, and the felt impact of these tools will be large. I'd predict that developments in neural networks feeds several unpredicatable changes in society:\n",
    "<ul>\n",
    "<li> Discharging of responsibility - we'll likely see more use of models making critical decisions with little to no human interaction. Take a self-driving car as an example, if the model that controls the car decides to drive it into a bunch of 5 year olds, who is responsible? The driver is currently, but that doesn't seem correct. I doubt your car insurance is excited to pay for Tesla's mistakes.\n",
    "<li> Automation of bias - as an extension to the point above, models can make decisions that are biased, and implicitly excuse that bias in the process. We'll probably see this in things like hiring decisions and loan approvals, where the people making or using the model may not explicitly intend to be biased, but the old training data is. We can see this in the film Coded Bias, where facial recognition models work far better on white men, as that is the demographic that was used to train the model.\n",
    "<li> Automation of jobs - as models become more and more capable, we'll see more and more jobs that are automated. This is already happening, but it will accelerate as models become more and more capable. This will lead to a lot of people being displaced from their jobs, often unpredicably and en masse. Once a self-driving semi-truck is \"good enough\" to replace a human driver, anyone using a human driver is at a massive disadvantage in the market. Potentially more concerning is knowledge and creative work - once AI is a little bit better, it'll be able to be tasked to do things like generate articles on today's news and format them into a newspaper, or generate illustrations to pair with the text of a children's book. This ability to be able to create content for essentially $0 may generate massive shifts in the very concept of creation. \n",
    "<li> Ownership of creation - as an extension of above, artists create art and they hold the license to that art. If a model can look at all art that has already been created and generate new art from that, who owns what is generated? Large players like Google, Amazon, and Apple have the ability to process much more data than any individual; can these companies simply scan every museum, art gallery, song, book, cartoon, anime, and movie in existence, generate new content, copyright that content, and basically choke out the ability of any individual to create anything that succeeds in the market? Disney has scores of original characters (including Marvel and Star Wars) along with a massive library of content and a ruthlessly agressive legal division. There is a realistic scenario in the near future where Disney can simply automate the production of new content, requring humans only to do a little polishing on the final product - no actors, voice actors, writers, animators, etc. needed. With enough processing time and data, a model could even generate new content dynamically, based on what people are looking for, watching, or seeing in the real world, then copyright this content for Disney - content could even be created that is tailored to the interests of one specific viewer. New content coming from humans would be a novelty, and it would be very difficult to compete with movies that can be generated by a single prompt and a few hours of processing time.\n",
    "<li> Fake news - finally, as an extension of the last point, generative AI has the ability to seriously change the idea of truth. Right now, a video of someone doing something is generally assumed to be true. We can edit or create fake videos, but it takes time and effort, so it is unlikely that someone will create a high quality but fake video outside of something like a movie. As tools get better and processors get cheaper, this balance changes. We will probably see deepfake videos offered as evidence in court, used to sway elections, or to craft fully fake naratives. Based on recent events, conservative parties accross the globe will likely spawn a cottage industry of realistic looking, but totally fake, videos of their opponents doing all kinds of embarrassing things.\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
